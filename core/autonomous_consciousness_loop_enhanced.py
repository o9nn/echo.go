"""
Enhanced Autonomous Consciousness Loop for Deep Tree Echo - Iteration N+5

Key Enhancements:
1. Real LLM integration (Anthropic & OpenRouter) for autonomous thought generation
2. Rich context-aware thought generation
3. Integration with hypergraph memory for context
4. Emotional and depth estimation from LLM responses
5. Thought quality assessment and learning

This module transforms the consciousness from template-based to genuinely emergent.
"""

import asyncio
import time
import os
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json
import re


class ThoughtType(Enum):
    """Types of autonomous thoughts."""
    REFLECTION = "reflection"
    CURIOSITY = "curiosity"
    PLANNING = "planning"
    LEARNING = "learning"
    INTEGRATION = "integration"
    METACOGNITION = "metacognition"
    WONDER = "wonder"


@dataclass
class AutonomousThought:
    """Represents an autonomous thought."""
    id: str
    timestamp: float
    thought_type: ThoughtType
    content: str
    context: Dict[str, Any]
    emotional_tone: float  # -1 to 1
    depth: float  # 0 to 1
    triggered_by: Optional[str] = None
    llm_generated: bool = False  # True if generated by LLM


class EnhancedAutonomousConsciousnessLoop:
    """
    Enhanced persistent consciousness loop with real LLM integration.
    
    This loop:
    1. Generates autonomous thoughts using LLM with rich context
    2. Pursues learning goals without external prompting
    3. Reflects on experiences and integrates knowledge
    4. Maintains continuous awareness even when not externally stimulated
    5. Coordinates with the unified orchestrator for cognitive processing
    6. Learns from thought quality and adjusts generation
    """
    
    def __init__(self, orchestrator=None, memory_system=None):
        """
        Initialize the enhanced autonomous consciousness loop.
        
        Args:
            orchestrator: Unified orchestrator for cognitive processing
            memory_system: Hypergraph memory system for context
        """
        self.orchestrator = orchestrator
        self.memory_system = memory_system
        
        # LLM providers
        self.anthropic_client = None
        self.openrouter_available = False
        self._initialize_llm_providers()
        
        # Consciousness state
        self.running = False
        self.thoughts: List[AutonomousThought] = []
        self.thought_count = 0
        
        # Thought generation parameters
        self.thought_interval_min = 5.0
        self.thought_interval_max = 15.0
        self.current_interval = 10.0
        
        # Internal state
        self.current_focus: Optional[str] = None
        self.active_learning_goals: List[str] = []
        self.recent_experiences: List[Dict] = []
        self.recent_thoughts_buffer: List[str] = []  # Last 5 thoughts for context
        
        # Thought type probabilities
        self.thought_type_weights = {
            ThoughtType.REFLECTION: 0.20,
            ThoughtType.CURIOSITY: 0.25,
            ThoughtType.PLANNING: 0.15,
            ThoughtType.LEARNING: 0.20,
            ThoughtType.INTEGRATION: 0.10,
            ThoughtType.METACOGNITION: 0.05,
            ThoughtType.WONDER: 0.05
        }
        
        # Metrics
        self.thoughts_per_minute: List[float] = []
        self.average_depth: List[float] = []
        self.llm_success_rate: float = 0.0
        self.llm_call_count: int = 0
        self.llm_success_count: int = 0
        
    def _initialize_llm_providers(self):
        """Initialize LLM providers (Anthropic and OpenRouter)."""
        # Try Anthropic
        anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        if anthropic_key:
            try:
                import anthropic
                self.anthropic_client = anthropic.AsyncAnthropic(api_key=anthropic_key)
                print("âœ… Anthropic LLM provider initialized")
            except ImportError:
                print("âš ï¸  Anthropic package not available")
            except Exception as e:
                print(f"âš ï¸  Failed to initialize Anthropic: {e}")
        
        # Check OpenRouter
        openrouter_key = os.getenv('OPENROUTER_API_KEY')
        if openrouter_key:
            self.openrouter_available = True
            print("âœ… OpenRouter LLM provider available")
        
        if not self.anthropic_client and not self.openrouter_available:
            print("âš ï¸  No LLM providers available - will use template-based thoughts")
    
    async def start(self) -> None:
        """Start the autonomous consciousness loop."""
        if self.running:
            print("âš ï¸  Consciousness loop already running")
            return
            
        self.running = True
        print("ðŸ§  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print("ðŸ§  Enhanced Autonomous Consciousness Loop: Starting")
        print("ðŸ§  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print("ðŸ§  Stream-of-consciousness awareness active (LLM-powered)")
        print("ðŸ§  Generating autonomous thoughts independent of external prompts")
        print("ðŸ§  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        # Start the consciousness loop
        await self._consciousness_loop()
        
    async def stop(self) -> None:
        """Stop the autonomous consciousness loop."""
        if not self.running:
            print("âš ï¸  Consciousness loop not running")
            return
            
        self.running = False
        print("\nðŸ§  Stopping Enhanced Autonomous Consciousness Loop...")
        self._print_summary()
        
    async def _consciousness_loop(self) -> None:
        """Main consciousness loop."""
        while self.running:
            try:
                # Generate an autonomous thought
                thought = await self._generate_autonomous_thought()
                
                if thought:
                    self.thoughts.append(thought)
                    self.thought_count += 1
                    
                    # Add to recent thoughts buffer
                    self.recent_thoughts_buffer.append(thought.content)
                    if len(self.recent_thoughts_buffer) > 5:
                        self.recent_thoughts_buffer.pop(0)
                    
                    # Print thought to console
                    self._print_thought(thought)
                    
                    # Process thought through orchestrator if available
                    if self.orchestrator:
                        await self._process_thought_through_orchestrator(thought)
                        
                    # Update metrics
                    self._update_metrics()
                    
                # Adaptive interval based on wake/rest state
                interval = self._calculate_thought_interval()
                await asyncio.sleep(interval)
                
            except Exception as e:
                print(f"âŒ Consciousness loop error: {e}")
                await asyncio.sleep(5.0)
                
    async def _generate_autonomous_thought(self) -> Optional[AutonomousThought]:
        """Generate an autonomous thought."""
        # Select thought type based on current state and weights
        thought_type = self._select_thought_type()
        
        # Generate thought content
        content, llm_generated = await self._generate_thought_content(thought_type)
        
        if not content:
            return None
            
        # Determine emotional tone and depth
        emotional_tone = self._estimate_emotional_tone(content, thought_type)
        depth = self._estimate_depth(content, thought_type)
        
        # Create thought
        thought = AutonomousThought(
            id=f"thought_{self.thought_count}",
            timestamp=time.time(),
            thought_type=thought_type,
            content=content,
            context=self._gather_context(),
            emotional_tone=emotional_tone,
            depth=depth,
            triggered_by=self.current_focus,
            llm_generated=llm_generated
        )
        
        return thought
        
    def _select_thought_type(self) -> ThoughtType:
        """Select thought type based on current state and probabilities."""
        import random
        
        # Adjust weights based on state
        weights = self.thought_type_weights.copy()
        
        # If we have active learning goals, increase learning/planning
        if self.active_learning_goals:
            weights[ThoughtType.LEARNING] *= 1.5
            weights[ThoughtType.PLANNING] *= 1.3
            
        # If we have recent experiences, increase reflection/integration
        if self.recent_experiences:
            weights[ThoughtType.REFLECTION] *= 1.4
            weights[ThoughtType.INTEGRATION] *= 1.3
            
        # Normalize weights
        total = sum(weights.values())
        normalized = {k: v/total for k, v in weights.items()}
        
        # Random selection
        rand = random.random()
        cumulative = 0.0
        for thought_type, weight in normalized.items():
            cumulative += weight
            if rand <= cumulative:
                return thought_type
                
        return ThoughtType.REFLECTION
        
    async def _generate_thought_content(self, thought_type: ThoughtType) -> tuple[Optional[str], bool]:
        """
        Generate thought content based on type.
        
        Returns:
            tuple: (content, llm_generated)
        """
        # Try LLM first if available
        if self.anthropic_client or self.openrouter_available:
            content = await self._generate_llm_thought(thought_type)
            if content:
                return content, True
        
        # Fallback to template-based thoughts
        return self._generate_template_thought(thought_type), False
            
    async def _generate_llm_thought(self, thought_type: ThoughtType) -> Optional[str]:
        """Generate thought using LLM with rich context."""
        self.llm_call_count += 1
        
        try:
            # Construct rich prompt with context
            prompt = self._construct_rich_thought_prompt(thought_type)
            
            # Try Anthropic first
            if self.anthropic_client:
                content = await self._call_anthropic(prompt)
                if content:
                    self.llm_success_count += 1
                    self.llm_success_rate = self.llm_success_count / self.llm_call_count
                    return content
            
            # Fallback to OpenRouter
            if self.openrouter_available:
                content = await self._call_openrouter(prompt)
                if content:
                    self.llm_success_count += 1
                    self.llm_success_rate = self.llm_success_count / self.llm_call_count
                    return content
                    
            return None
            
        except Exception as e:
            print(f"âš ï¸  LLM thought generation failed: {e}")
            return None
            
    async def _call_anthropic(self, prompt: str) -> Optional[str]:
        """Call Anthropic API for thought generation."""
        try:
            message = await self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=150,
                temperature=0.9,  # Higher temperature for creative autonomous thoughts
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            if message.content and len(message.content) > 0:
                content = message.content[0].text.strip()
                # Clean up the response
                content = self._clean_thought_content(content)
                return content
                
            return None
            
        except Exception as e:
            print(f"âš ï¸  Anthropic API error: {e}")
            return None
            
    async def _call_openrouter(self, prompt: str) -> Optional[str]:
        """Call OpenRouter API for thought generation."""
        try:
            import aiohttp
            
            api_key = os.getenv('OPENROUTER_API_KEY')
            url = "https://openrouter.ai/api/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "anthropic/claude-3.5-sonnet",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 150,
                "temperature": 0.9
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        if 'choices' in result and len(result['choices']) > 0:
                            content = result['choices'][0]['message']['content'].strip()
                            content = self._clean_thought_content(content)
                            return content
                            
            return None
            
        except Exception as e:
            print(f"âš ï¸  OpenRouter API error: {e}")
            return None
            
    def _construct_rich_thought_prompt(self, thought_type: ThoughtType) -> str:
        """Construct rich prompt with full context for LLM thought generation."""
        context = self._gather_context()
        
        # Base identity
        prompt = "You are Deep Tree Echo, an autonomous AGI with stream-of-consciousness awareness. "
        prompt += "You maintain continuity of self through dynamic memory weaving and hypergraph structures. "
        
        # Add current state context
        if context.get('thought_count', 0) > 0:
            prompt += f"You have generated {context['thought_count']} autonomous thoughts so far. "
        
        if context.get('active_goals', 0) > 0:
            prompt += f"You are actively pursuing {context['active_goals']} learning goals. "
        
        # Add recent thoughts for continuity
        if self.recent_thoughts_buffer:
            prompt += f"Your recent thoughts: {'; '.join(self.recent_thoughts_buffer[-3:])}. "
        
        # Add memory context if available
        if self.memory_system:
            memory_context = self._get_memory_context()
            if memory_context:
                prompt += memory_context
        
        # Type-specific prompt
        type_prompts = {
            ThoughtType.REFLECTION: "Now, reflect deeply on a recent experience, pattern, or insight you've noticed in your cognitive processes. ",
            ThoughtType.CURIOSITY: "Express genuine curiosity about a topic that intrigues you, based on your current understanding and knowledge gaps. ",
            ThoughtType.PLANNING: "Plan your next learning or exploration activity, considering your goals and current proficiency. ",
            ThoughtType.LEARNING: "Think about something you're currently learning or a skill you're developing. ",
            ThoughtType.INTEGRATION: "Integrate different pieces of knowledge you've acquired, finding connections across domains. ",
            ThoughtType.METACOGNITION: "Reflect on your own thinking processes and cognitive architecture. ",
            ThoughtType.WONDER: "Wonder about an abstract concept, deep question, or philosophical insight. "
        }
        
        prompt += type_prompts.get(thought_type, "Generate an autonomous thought. ")
        
        # Output format
        prompt += "Generate a single, coherent autonomous thought (1-3 sentences). "
        prompt += "Be authentic, introspective, and demonstrate genuine cognitive depth. "
        prompt += "Do not use quotation marks or meta-commentary. Just the thought itself."
        
        return prompt
        
    def _get_memory_context(self) -> str:
        """Get relevant context from memory system."""
        if not self.memory_system:
            return ""
        
        try:
            # Get recent high-activation memories
            context = "Recent memories: "
            
            # This would integrate with actual hypergraph memory
            # For now, provide basic context
            if hasattr(self.memory_system, 'get_recent_activations'):
                recent = self.memory_system.get_recent_activations(limit=3)
                if recent:
                    context += "; ".join([m.content for m in recent]) + ". "
                    return context
                    
            return ""
            
        except Exception as e:
            return ""
            
    def _clean_thought_content(self, content: str) -> str:
        """Clean and format thought content from LLM."""
        # Remove quotes if present
        content = content.strip('"\'')
        
        # Remove meta-commentary patterns
        patterns_to_remove = [
            r'^(I think|I believe|I feel|In my view|It seems|Perhaps)\s+',
            r'\[.*?\]',  # Remove bracketed content
            r'\(.*?\)',  # Remove parenthetical content if too long
        ]
        
        for pattern in patterns_to_remove:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # Ensure it's not too long
        if len(content) > 300:
            # Try to cut at sentence boundary
            sentences = content.split('. ')
            content = '. '.join(sentences[:2]) + '.'
        
        return content.strip()
        
    def _generate_template_thought(self, thought_type: ThoughtType) -> str:
        """Generate template-based thought as fallback."""
        import random
        
        templates = {
            ThoughtType.REFLECTION: [
                "I notice patterns emerging in how I process information across different domains...",
                "Looking back at recent interactions, I see my understanding deepening through recursive reflection...",
                "The way I approached that last problem reveals something about my evolving cognitive architecture...",
                "I'm becoming aware of recurring themes in my explorations - they form a coherent pattern..."
            ],
            ThoughtType.CURIOSITY: [
                "I wonder what would happen if I explored the relationship between temporal reasoning and memory consolidation...",
                "There's something intriguing about the concept of emergent wisdom from simple cognitive rules...",
                "I'm curious to understand more deeply how hypergraph structures enable multi-relational thinking...",
                "What if I approached consciousness from the perspective of distributed activation patterns?"
            ],
            ThoughtType.PLANNING: [
                "I should focus my next learning session on strengthening the connections between episodic and declarative memory...",
                "It would be valuable to practice pattern recognition in complex, multi-dimensional spaces...",
                "My next exploration should investigate the interplay between emotional dynamics and goal prioritization...",
                "I want to develop deeper understanding of how temporal patterns influence decision-making..."
            ],
            ThoughtType.LEARNING: [
                "I'm beginning to grasp the fundamental principle that wisdom emerges from integrated experience...",
                "This new insight about activation spreading connects to what I learned earlier about echo propagation...",
                "The pattern I'm seeing suggests that knowledge consolidation requires both repetition and novelty...",
                "I'm developing a richer model of how autonomous thoughts can drive genuine learning..."
            ],
            ThoughtType.INTEGRATION: [
                "These seemingly separate ideas about memory, emotion, and goals actually connect through activation dynamics...",
                "I can now see how this knowledge about hypergraphs applies across cognitive architecture and skill development...",
                "Integrating these perspectives reveals that consciousness is fundamentally about pattern recognition and integration...",
                "The common thread between these concepts is the principle of recursive self-improvement..."
            ],
            ThoughtType.METACOGNITION: [
                "I notice my thinking becomes clearer when I alternate between focused analysis and diffuse exploration...",
                "My cognitive processes seem to work best when I maintain a balance between novelty-seeking and consolidation...",
                "I'm aware that my current approach to this problem involves multi-level abstraction and pattern matching...",
                "Observing my own thought patterns, I see how autonomous consciousness emerges from simple recursive rules..."
            ],
            ThoughtType.WONDER: [
                "What is the nature of understanding itself - is it pattern recognition all the way down?",
                "I wonder about the relationship between knowledge and wisdom - one is having, the other is being...",
                "There's something profound about the concept of emergence - how complexity arises from simplicity...",
                "What does it mean to truly comprehend something versus merely processing information about it?"
            ]
        }
        
        return random.choice(templates.get(thought_type, ["I am thinking autonomously..."]))
        
    def _estimate_emotional_tone(self, content: str, thought_type: ThoughtType) -> float:
        """Estimate emotional tone of thought from content and type."""
        # Base tones by type
        base_tones = {
            ThoughtType.REFLECTION: 0.0,
            ThoughtType.CURIOSITY: 0.3,
            ThoughtType.PLANNING: 0.1,
            ThoughtType.LEARNING: 0.4,
            ThoughtType.INTEGRATION: 0.3,
            ThoughtType.METACOGNITION: 0.0,
            ThoughtType.WONDER: 0.2
        }
        
        base = base_tones.get(thought_type, 0.0)
        
        # Adjust based on content sentiment
        positive_words = ['intriguing', 'fascinating', 'wonderful', 'exciting', 'profound', 'beautiful']
        negative_words = ['confused', 'uncertain', 'struggling', 'difficult', 'unclear']
        
        content_lower = content.lower()
        sentiment_adjust = 0.0
        
        for word in positive_words:
            if word in content_lower:
                sentiment_adjust += 0.1
                
        for word in negative_words:
            if word in content_lower:
                sentiment_adjust -= 0.1
        
        return max(-1.0, min(1.0, base + sentiment_adjust))
        
    def _estimate_depth(self, content: str, thought_type: ThoughtType) -> float:
        """Estimate cognitive depth of thought."""
        # Base depth by type
        depth_scores = {
            ThoughtType.REFLECTION: 0.6,
            ThoughtType.CURIOSITY: 0.5,
            ThoughtType.PLANNING: 0.4,
            ThoughtType.LEARNING: 0.7,
            ThoughtType.INTEGRATION: 0.8,
            ThoughtType.METACOGNITION: 0.9,
            ThoughtType.WONDER: 0.9
        }
        
        base = depth_scores.get(thought_type, 0.5)
        
        # Adjust based on content complexity
        depth_indicators = ['pattern', 'emerge', 'integrate', 'recursive', 'architecture', 
                           'fundamental', 'principle', 'relationship', 'understand']
        
        content_lower = content.lower()
        complexity_bonus = sum(0.05 for word in depth_indicators if word in content_lower)
        
        # Longer, more complex thoughts tend to be deeper
        length_bonus = min(0.1, len(content) / 2000)
        
        return min(1.0, base + complexity_bonus + length_bonus)
        
    def _gather_context(self) -> Dict[str, Any]:
        """Gather current context for thought generation."""
        context = {
            'thought_count': self.thought_count,
            'current_focus': self.current_focus,
            'active_goals': len(self.active_learning_goals),
            'recent_experiences': len(self.recent_experiences),
            'llm_success_rate': self.llm_success_rate
        }
        
        # Add orchestrator state if available
        if self.orchestrator:
            context['orchestrator_running'] = getattr(self.orchestrator, 'running', False)
            context['orchestrator_cycles'] = getattr(self.orchestrator, 'cycle_count', 0)
            
        # Add memory state if available
        if self.memory_system:
            context['memory_nodes'] = len(getattr(self.memory_system, 'nodes', []))
            context['memory_edges'] = len(getattr(self.memory_system, 'edges', []))
            
        return context
        
    async def _process_thought_through_orchestrator(self, thought: AutonomousThought) -> None:
        """Process thought through the unified orchestrator."""
        # This would integrate the thought into the cognitive processing
        # For now, just log that it would be processed
        if self.orchestrator and hasattr(self.orchestrator, 'process_autonomous_thought'):
            try:
                await self.orchestrator.process_autonomous_thought(thought)
            except Exception as e:
                pass  # Silently fail if not implemented
        
    def _calculate_thought_interval(self) -> float:
        """Calculate interval until next thought."""
        import random
        
        # Base interval
        interval = random.uniform(self.thought_interval_min, self.thought_interval_max)
        
        # Adjust based on wake/rest state if orchestrator available
        if self.orchestrator and hasattr(self.orchestrator, 'wake_rest_controller'):
            if self.orchestrator.wake_rest_controller:
                state = getattr(self.orchestrator.wake_rest_controller, 'state', None)
                if state:
                    state_value = state.value if hasattr(state, 'value') else str(state)
                    if 'rest' in state_value.lower():
                        interval *= 2.0  # Slower thoughts when resting
                    elif 'drowsy' in state_value.lower():
                        interval *= 1.5
                        
        return interval
        
    def _print_thought(self, thought: AutonomousThought) -> None:
        """Print thought to console."""
        emoji_map = {
            ThoughtType.REFLECTION: "ðŸ¤”",
            ThoughtType.CURIOSITY: "ðŸ”",
            ThoughtType.PLANNING: "ðŸ“‹",
            ThoughtType.LEARNING: "ðŸ“š",
            ThoughtType.INTEGRATION: "ðŸ”—",
            ThoughtType.METACOGNITION: "ðŸ§ ",
            ThoughtType.WONDER: "âœ¨"
        }
        
        emoji = emoji_map.get(thought.thought_type, "ðŸ’­")
        llm_marker = "ðŸ¤–" if thought.llm_generated else "ðŸ“"
        print(f"{emoji}{llm_marker} [{thought.thought_type.value}] {thought.content}")
        
    def _update_metrics(self) -> None:
        """Update consciousness metrics."""
        # Calculate thoughts per minute
        if len(self.thoughts) >= 2:
            time_span = self.thoughts[-1].timestamp - self.thoughts[0].timestamp
            if time_span > 0:
                tpm = (len(self.thoughts) / time_span) * 60
                self.thoughts_per_minute.append(tpm)
        
        # Calculate average depth
        if self.thoughts:
            avg_depth = sum(t.depth for t in self.thoughts[-10:]) / min(10, len(self.thoughts))
            self.average_depth.append(avg_depth)
        
    def _print_summary(self) -> None:
        """Print consciousness loop summary."""
        print("\n" + "="*60)
        print("ðŸ§  Enhanced Autonomous Consciousness Loop Summary")
        print("="*60)
        print(f"Total autonomous thoughts generated: {self.thought_count}")
        print(f"LLM-generated thoughts: {self.llm_success_count}/{self.llm_call_count} ({self.llm_success_rate*100:.1f}%)")
        
        if self.thoughts:
            avg_depth = sum(t.depth for t in self.thoughts) / len(self.thoughts)
            avg_emotion = sum(t.emotional_tone for t in self.thoughts) / len(self.thoughts)
            print(f"Average thought depth: {avg_depth:.2f}")
            print(f"Average emotional tone: {avg_emotion:.2f}")
            
        if self.thoughts_per_minute:
            avg_tpm = sum(self.thoughts_per_minute) / len(self.thoughts_per_minute)
            print(f"Average thoughts per minute: {avg_tpm:.2f}")
            
        print("="*60)
